{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract images from label directories into training and test sets\n",
    "def extract_label_images(path,label,training_images,training_labels,train_value,test_images,test_labels,test_value):\n",
    "    new_path = path + \"/\" +label\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    ## resize images to 64x64 grayscale \n",
    "    for file in os.listdir(new_path):\n",
    "    # grab only 10 images from label directory for testing purposes \n",
    "        if (train_count < train_value):\n",
    "            img = cv2.cvtColor(cv2.imread(os.path.join(new_path+ \"/\" +file)), cv2.COLOR_BGR2GRAY)\n",
    "            #print(img.shape)\n",
    "            img = cv2.resize(img,(64,64))\n",
    "            training_images.append(img)\n",
    "            training_labels.append(label)\n",
    "            train_count = train_count + 1 \n",
    "            \n",
    "        elif (test_count < test_value):\n",
    "            img = cv2.cvtColor(cv2.imread(os.path.join(new_path+ \"/\" +file)), cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img,(64,64))\n",
    "            test_images.append(img)\n",
    "            test_labels.append(label)\n",
    "            test_count = test_count + 1 \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    print(\"FINISH EXTRACTING LABEL : \", label)\n",
    "    print(\"NUMBER OF SAMPLE FOR LABEL :\", train_count)\n",
    "    return training_images,test_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLabels(path):\n",
    "    label_list = []\n",
    "    for file in os.listdir(path):\n",
    "        label_list.append(\"\"+file)\n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHogFeatures(images):\n",
    "    cell_size = (4, 4)  # h x w in pixels\n",
    "    block_size = (4, 4)  # h x w in cells\n",
    "    nbins = 8  # number of orientation bins\n",
    "    \n",
    "    hog = cv2.HOGDescriptor(_winSize=(images[0].shape[1] // cell_size[1] * cell_size[1],\n",
    "                                          images[0].shape[0] // cell_size[0] * cell_size[0]),\n",
    "                                _blockSize=(block_size[1] * cell_size[1],\n",
    "                                            block_size[0] * cell_size[0]),\n",
    "                                _blockStride=(cell_size[1], cell_size[0]),\n",
    "                                _cellSize=(cell_size[1], cell_size[0]),\n",
    "                                _nbins=nbins)\n",
    "    \n",
    "    n_cells = (images[0].shape[0] // cell_size[0], images[0].shape[1] // cell_size[1])\n",
    "    \n",
    "    listOfHogFeatures = []\n",
    "    for img in images:\n",
    "        # create HoG Object\n",
    "        # winSize is the size of the image cropped to an multiple of the cell size\n",
    "        \n",
    "\n",
    "        # Compute HoG features\n",
    "        hog_feats = hog.compute(img)\\\n",
    "                       .reshape(n_cells[1] - block_size[1] + 1,\n",
    "                                n_cells[0] - block_size[0] + 1,\n",
    "                                block_size[0], block_size[1], nbins) \\\n",
    "                       .transpose((1, 0, 2, 3, 4))  # index blocks by rows first\n",
    "\n",
    "        # hog_feats now contains the gradient amplitudes for each direction,for each cell of its group for each group.\n",
    "        # Indexing is by rows then columns.\n",
    "        # computation for BlockNorm\n",
    "        gradients = np.full((n_cells[0], n_cells[1], 8), 0, dtype=float)\n",
    "        cell_count = np.full((n_cells[0], n_cells[1], 1), 0, dtype=int)\n",
    "\n",
    "        for off_y in range(block_size[0]):\n",
    "            for off_x in range(block_size[1]):\n",
    "                gradients[off_y:n_cells[0] - block_size[0] + off_y + 1,\n",
    "                          off_x:n_cells[1] - block_size[1] + off_x + 1] += \\\n",
    "                    hog_feats[:, :, off_y, off_x, :]\n",
    "                cell_count[off_y:n_cells[0] - block_size[0] + off_y + 1,\n",
    "                           off_x:n_cells[1] - block_size[1] + off_x + 1] += 1\n",
    "\n",
    "        # Average gradients\n",
    "        gradients /= cell_count\n",
    "        # turn gradient 32,32,8 3D array to 1D for training \n",
    "        reshaped_gradients = gradients.ravel()\n",
    "        listOfHogFeatures.append(reshaped_gradients)\n",
    "        \n",
    "    return listOfHogFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH EXTRACTING LABEL :  articulated_truck\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  background\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  bicycle\n",
      "NUMBER OF SAMPLE FOR LABEL : 2284\n",
      "FINISH EXTRACTING LABEL :  bus\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  car\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  motorcycle\n",
      "NUMBER OF SAMPLE FOR LABEL : 1982\n",
      "FINISH EXTRACTING LABEL :  non-motorized_vehicle\n",
      "NUMBER OF SAMPLE FOR LABEL : 1751\n",
      "FINISH EXTRACTING LABEL :  pedestrian\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  pickup_truck\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  single_unit_truck\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "FINISH EXTRACTING LABEL :  work_van\n",
      "NUMBER OF SAMPLE FOR LABEL : 5000\n",
      "46017\n",
      "46017\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "training_set_path= r\"./MIO-TCD-Classification/train\"\n",
    "labels = extractLabels(training_set_path)\n",
    "train_images = []\n",
    "train_labels = []\n",
    "train_value = 5000\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_value = 1\n",
    "\n",
    "## Extracting 64x64 gray scale images from each label directory\n",
    "for label in labels:\n",
    "    extract_label_images(training_set_path, label, train_images, train_labels, train_value, test_images, test_labels, test_value)\n",
    "\n",
    "print(len(train_images))\n",
    "print(len(train_labels))\n",
    "print(len(test_images))\n",
    "print(len(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "## extract hog features of each images of labels \n",
    "training_img_features = getHogFeatures(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45556, 2048)\n",
      "(461, 2048)\n",
      "(45556,)\n",
      "(461,)\n"
     ]
    }
   ],
   "source": [
    "training_img_features = np.array(training_img_features)\n",
    "train_labels = np.array(train_labels)\n",
    "# randomize training set.\n",
    "X_train, X_test, label_train, Y_test = train_test_split(training_img_features, train_labels, train_size=0.99, test_size=0.01)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(label_train.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY : 0.7684372256365233\n",
      "PRECISION : 0.7684372256365233\n",
      "RECALL :  0.7684372256365233\n",
      "ACCURACY : 0.7570237050043899\n",
      "PRECISION : 0.7570237050043899\n",
      "RECALL :  0.7570237050043899\n",
      "ACCURACY : 0.754828797190518\n",
      "PRECISION : 0.754828797190518\n",
      "RECALL :  0.754828797190518\n",
      "ACCURACY : 0.7456101843722563\n",
      "PRECISION : 0.7456101843722563\n",
      "RECALL :  0.7456101843722563\n",
      "ACCURACY : 0.7480245829675154\n",
      "PRECISION : 0.7480245829675154\n",
      "RECALL :  0.7480245829675154\n",
      "ACCURACY : 0.7561457418788411\n",
      "PRECISION : 0.7561457418788411\n",
      "RECALL :  0.7561457418788411\n",
      "ACCURACY : 0.7554335894621296\n",
      "PRECISION : 0.7554335894621296\n",
      "RECALL :  0.7554335894621296\n",
      "ACCURACY : 0.7523600439077937\n",
      "PRECISION : 0.7523600439077937\n",
      "RECALL :  0.7523600439077937\n",
      "ACCURACY : 0.750384193194292\n",
      "PRECISION : 0.750384193194292\n",
      "RECALL :  0.750384193194292\n",
      "ACCURACY : 0.748847420417124\n",
      "PRECISION : 0.748847420417124\n",
      "RECALL :  0.748847420417124\n",
      "ACCURACY DATA : \n",
      "[0.7684372256365233, 0.7570237050043899, 0.754828797190518, 0.7456101843722563, 0.7480245829675154, 0.7561457418788411, 0.7554335894621296, 0.7523600439077937, 0.750384193194292, 0.748847420417124]\n",
      "PRECISION DATA : \n",
      "[0.7684372256365233, 0.7570237050043899, 0.754828797190518, 0.7456101843722563, 0.7480245829675154, 0.7561457418788411, 0.7554335894621296, 0.7523600439077937, 0.750384193194292, 0.748847420417124]\n",
      "RECALL : \n",
      "[0.7684372256365233, 0.7570237050043899, 0.754828797190518, 0.7456101843722563, 0.7480245829675154, 0.7561457418788411, 0.7554335894621296, 0.7523600439077937, 0.750384193194292, 0.748847420417124]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf_splitter = KFold(n_splits=10)\n",
    "\n",
    "valid_label_prediction_accuracy = []\n",
    "valid_label_prediction_precision = []\n",
    "valid_label_prediction_recall = []\n",
    "\n",
    "# kfold cross validation\n",
    "for train_index, valid_index in kf_splitter.split(X_train):\n",
    "\n",
    "    train_set = X_train[train_index, :]\n",
    "    t_label = label_train[train_index]\n",
    "    \n",
    "    # Get our validation data in this fold\n",
    "    valid_set = X_train[valid_index, :]\n",
    "    v_label = label_train[valid_index]\n",
    "    \n",
    "    # Train KNN Classfier\n",
    "    model.fit(train_set, t_label)\n",
    "    \n",
    "    # Get our prediction for validation:\n",
    "    prediction_valid = model.predict(valid_set)\n",
    "    \n",
    "    accuracy = accuracy_score(v_label, prediction_valid)\n",
    "    print(\"ACCURACY :\", accuracy)\n",
    "    valid_label_prediction_accuracy.append(accuracy)\n",
    "    \n",
    "    precision = precision_score(v_label, prediction_valid, average='micro')\n",
    "    print(\"PRECISION :\", precision)\n",
    "    valid_label_prediction_precision.append(precision)\n",
    "    \n",
    "    recall = recall_score(v_label, prediction_valid, average='micro')\n",
    "    print(\"RECALL : \", recall)\n",
    "    valid_label_prediction_recall.append(recall)\n",
    "    \n",
    "print(\"ACCURACY DATA : \")\n",
    "print(valid_label_prediction_accuracy)\n",
    "print(\"PRECISION DATA : \")\n",
    "print(valid_label_prediction_precision)\n",
    "print(\"RECALL : \")\n",
    "print(valid_label_prediction_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE ACCURACY SCORE :  75.37095484031383 %\n",
      "STANDARD DEVIATION :  0.6103164364900331 %\n",
      "AVERAGE RECALL SCORE :  75.37095484031383 %\n",
      "AVERAGE PRECISION SCORE :  75.37095484031383 %\n"
     ]
    }
   ],
   "source": [
    "print(\"AVERAGE ACCURACY SCORE : \", str(np.mean(valid_label_prediction_accuracy) * 100) + \" %\")\n",
    "print(\"STANDARD DEVIATION : \", str(np.std(valid_label_prediction_accuracy) * 100) + \" %\")\n",
    "print(\"AVERAGE RECALL SCORE : \", str(np.mean(valid_label_prediction_recall) * 100) + \" %\")\n",
    "print(\"AVERAGE PRECISION SCORE : \", str(np.mean(valid_label_prediction_precision) * 100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1901cfd43c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADT5JREFUeJzt3VuM3PV5xvHn2dn1kUDsuEGJbbBREC2lihxtEWAVRThRnUOhF61kKqIkqupeNAmkUSMSqeKmF5UaIXKBolqEBCkIVDmWgiIUkoZEUSVwsxirwSxpLBzMYju2wTERBPb09mKHdr2x8XT+7xys9/uRrN0Z/nrm3V2e/c/ht79xRAhALSODHgBA/1F8oCCKDxRE8YGCKD5QEMUHCqL4QEEUHyiI4gMFjfbzxi5eOxqXrh9Lyzv+3DvSsiQpZmfTsmynZUlS+grL5Pmyv95sqd+/VSvysiTp9d+mRb0Rr2k63jzvD6Ovxb90/Zju/s770vLu3XpjWpYkzZ14OS1rZFneLzhJmp+eSc1zq5Wbl/z1ZovE719cc1ValiR532Ra1pOzj3V0HHf1gYIoPlAQxQcKovhAQRQfKKhR8W1vt/1z2wdt35k1FIDe6rr4tluS7pX0EUlXS7rV9tVZgwHonSZn/GslHYyI5yNiWtLDkm7JGQtALzUp/npJLy66PNW+7gy2d9qesD1x+pW5BjcHIEuT4p9tWeDvrIuMiF0RMR4R45eszV0tBqA7TYo/JWnjossbJB1pNg6AfmhS/J9KutL2ZtvLJO2Q9EjOWAB6qes/0omIWdufkfSYpJak+yPiQNpkAHqm0V/nRcSjkh5NmgVAn7ByDyiI4gMFUXygIIoPFOR+vlvuJaPr4vqL8lb1/st/dbbNUKf+4Y9vTsuaP3UqLUuSYi531ePI8uWpeZn7FUrS9Affn5o39vj+tKz0bctaeeffJ994VKfnXz7vnnuc8YGCKD5QEMUHCqL4QEEUHyiI4gMFUXygIIoPFETxgYIoPlAQxQcKovhAQRQfKIjiAwVRfKAgig8URPGBgig+UBDFBwpq9IYa/18xN6+5V19Ny/v7zTekZUnSyZ3vS8ta969PpGVdCLL33FsxcTA1b24+b8/CSMySpJhJzOpwD03O+EBBFB8oiOIDBVF8oCCKDxTUdfFtb7T9I9uTtg/Yvj1zMAC90+TlvFlJX4iIfbbfIekp2z+IiGeTZgPQI12f8SPiaETsa3/+G0mTktZnDQagd1Ie49veJGmLpL0ZeQB6q/HKPdsXSfq2pDsi4neW5dneKWmnJK3QqqY3ByBBozO+7TEtlP7BiNhztmMiYldEjEfE+Jhy35oZQHeaPKtvSV+XNBkRd+eNBKDXmpzxt0r6hKSbbO9v//to0lwAeqjrx/gR8R+SnDgLgD5h5R5QEMUHCqL4QEEUHyior1tveWxUo+suTcuLiy9Ky5KkdbueTMt67Mj+tCxJ2n7ZeGpe9lZZrXetTc3TfGdbSA3C6OUbcwPn5tOifGyso+M44wMFUXygIIoPFETxgYIoPlAQxQcKovhAQRQfKIjiAwVRfKAgig8URPGBgig+UBDFBwqi+EBBFB8oiOIDBVF8oCCKDxTU1z33YmZWs786nheYmZXsT9dvSc2759BPUvPuuOJPUvPmXjmVmudly1LzRjdfnpY1+8JUWpYktdZckhcWne3fxxkfKIjiAwVRfKAgig8URPGBgig+UFDj4ttu2X7a9nczBgLQexln/NslTSbkAOiTRsW3vUHSxyTdlzMOgH5oesa/R9IXJZ1zuZDtnbYnbE/M6M2GNwcgQ9fFt/1xSccj4qm3Oy4idkXEeESMj2l5tzcHIFGTM/5WSTfb/qWkhyXdZPtbKVMB6Kmuix8RX4qIDRGxSdIOSY9HxG1pkwHoGV7HBwpK+bPciPixpB9nZAHoPc74QEEUHyiI4gMFUXygoL7uuSdJiuj7TXbKy/MWGMWbuasU79h0Q2reqU9dm5q35ptPpOZl77k3e+iF1LxMcy+/kpYVMdfRcZzxgYIoPlAQxQcKovhAQRQfKIjiAwVRfKAgig8URPGBgig+UBDFBwqi+EBBFB8oiOIDBVF8oCCKDxRE8YGCKD5QEMUHCur/nnsjrb7fZKdS98mz87Kk9L0K1zzwZGqet/xhat78/mdT81IN+c+2E5zxgYIoPlAQxQcKovhAQRQfKKhR8W2/0/Zu28/ZnrR9fdZgAHqn6ct5X5X0vYj4C9vLJK1KmAlAj3VdfNsXS7pR0qckKSKmJU3njAWgl5rc1b9C0glJ37D9tO37bK9OmgtADzUp/qikD0j6WkRskfSapDuXHmR7p+0J2xMzyn0HWQDdaVL8KUlTEbG3fXm3Fn4RnCEidkXEeESMjynvbagBdK/r4kfEMUkv2r6qfdU2SUO8wBrAW5o+q/9ZSQ+2n9F/XtKnm48EoNcaFT8i9ksaT5oFQJ+wcg8oiOIDBVF8oCCKDxRE8YGC+r/n3vxcWpRHc8f3iry/MZp//fW0rAtBJO+Rd/KRK1Pz1v3Zf+eFDWCPvGyc8YGCKD5QEMUHCqL4QEEUHyiI4gMFUXygIIoPFETxgYIoPlAQxQcKovhAQRQfKIjiAwVRfKAgig8URPGBgig+UBDFBwrq6557HhnRyMq8fe1iejotKz1vpJWXJan1rrWpefOnTqXmqZX79b57x0upef906D/Tsv5xfHtaliR55cq8rGNjHR3HGR8oiOIDBVF8oCCKDxRE8YGCGhXf9udtH7D9jO2HbK/IGgxA73RdfNvrJX1O0nhEXCOpJWlH1mAAeqfpXf1RSSttj0paJelI85EA9FrXxY+IlyR9RdJhSUclnY6I7y89zvZO2xO2J6bjje4nBZCmyV39NZJukbRZ0nslrbZ929LjImJXRIxHxPgyngIAhkKTu/ofknQoIk5ExIykPZJuyBkLQC81Kf5hSdfZXmXbkrZJmswZC0AvNXmMv1fSbkn7JP2snbUraS4APdTor/Mi4i5JdyXNAqBPWLkHFETxgYIoPlAQxQcK6uvWW2q1NLJ2TVrc/Klfp2VJ0vxv81YWeiz3Wzt38mRqnpz7O791yerUvHjt9dS8L2++Ni3rpT3vScuSpMv+5mheWMx3dBhnfKAgig8URPGBgig+UBDFBwqi+EBBFB8oiOIDBVF8oCCKDxRE8YGCKD5QEMUHCqL4QEEUHyiI4gMFUXygIIoPFETxgYL6uudezMxo9sixtDyPOC1LUsf7lXUUNT2dliVJbrVS82JuLjVv/vSrqXkxH6l5Xr48Leuyvz2eliVJf/Tvr6Rl7f+rzn6unPGBgig+UBDFBwqi+EBBFB8o6LzFt32/7eO2n1l03VrbP7D9i/bHvLfHAdBznZzxvylp+5Lr7pT0w4i4UtIP25cBXCDOW/yI+ImkpS803iLpgfbnD0j68+S5APRQt4/xL42Io5LU/vjuvJEA9FrPV+7Z3ilppySt0Kpe3xyADnR7xv+V7fdIUvvjOdcwRsSuiBiPiPEx5S2bBNC9bov/iKRPtj//pKTv5IwDoB86eTnvIUlPSLrK9pTtv5b0z5I+bPsXkj7cvgzgAnHex/gRces5/tO25FkA9Akr94CCKD5QEMUHCqL4QEEUHyjIEbl7m73tjdknJL3QwaHrJJ3s8TjdGubZpOGeb5hnk4Z7vk5nuzwifu98B/W1+J2yPRER44Oe42yGeTZpuOcb5tmk4Z4vezbu6gMFUXygoGEt/q5BD/A2hnk2abjnG+bZpOGeL3W2oXyMD6C3hvWMD6CHhqr4trfb/rntg7aHah8/2xtt/8j2pO0Dtm8f9ExL2W7Zftr2dwc9y1K232l7t+3n2t/D6wc901tsf779M33G9kO2Vwx4np5vcDs0xbfdknSvpI9IulrSrbavHuxUZ5iV9IWI+ANJ10n6uyGbT5JulzQ56CHO4auSvhcRvy/p/RqSOW2vl/Q5SeMRcY2klqQdg52q9xvcDk3xJV0r6WBEPB8R05Ie1sKmnkMhIo5GxL7257/Rwv+46wc71f+xvUHSxyTdN+hZlrJ9saQbJX1dkiJiOiJ+PdipzjAqaaXtUUmrJB0Z5DD92OB2mIq/XtKLiy5PaYiKtZjtTZK2SNo72EnOcI+kL0rKe8vfPFdIOiHpG+2HIvfZXj3ooSQpIl6S9BVJhyUdlXQ6Ir4/2KnOKnWD22Eq/tne83roXnKwfZGkb0u6IyJy3xu6S7Y/Lul4RDw16FnOYVTSByR9LSK2SHpNQ/JeDO3HyrdI2izpvZJW275tsFP13jAVf0rSxkWXN2jAd7mWsj2mhdI/GBF7Bj3PIlsl3Wz7l1p4iHST7W8NdqQzTEmaioi37iHt1sIvgmHwIUmHIuJERMxI2iPphgHPdDYdb3DbiWEq/k8lXWl7s+1lWniC5ZEBz/S/bFsLj1EnI+LuQc+zWER8KSI2RMQmLXzfHo+IoTlrRcQxSS/avqp91TZJzw5wpMUOS7rO9qr2z3ibhuSJxyVSN7jt+b76nYqIWdufkfSYFp5ZvT8iDgx4rMW2SvqEpJ/Z3t++7ssR8egAZ7qQfFbSg+1f6s9L+vSA55EkRcRe27sl7dPCKzdPa8Ar+Nob3H5Q0jrbU5Lu0sKGtv/W3uz2sKS/bHQbrNwD6hmmu/oA+oTiAwVRfKAgig8URPGBgig+UBDFBwqi+EBB/wNfGCt/ovgYHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(prediction_valid, v_label)\n",
    "plt.figure()\n",
    "plt.imshow(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVING MODEL\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, 'filename.pk1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
